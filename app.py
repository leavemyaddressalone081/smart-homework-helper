import streamlit as st
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import re
import torch

# Page config
st.set_page_config(page_title="üìö Safe Homework Helper", layout="centered")

# Title
st.title("üìö Safe Homework Helper AI")
st.write("Ask me any homework question! I help with all subjects in a clear, step-by-step way üòâ.")

# Inappropriate content filter
INAPPROPRIATE_WORDS = [
    # Suicide & self-harm
    "suicidal", "kill myself", "killmyself", "kms", "end my life",
    "hang myself", "cut myself", "selfharm", "slit wrist",
    
    # Gore & violence
    "gore", "blood", "bloody", "killing", "stab", "stabbing",
    "shoot", "shooting", "gun", "knife", "weapon", "mutilate",
    
    # Sexual content
    "sex", "sexy", "porn", "porno", "pornography", "xxx", "nude", "naked",
    "strip", "stripper", "masturbate", "masturbation", "orgasm",
    "boob", "boobs", "tit", "tits", "breast", "breasts", "bobs", "boobies",
    "dick", "cock", "penis", "vagina", "pussy", "puss", "ass", "butt",
    "anal", "oral", "blowjob", "handjob", "rape", "molest", "pedophile",
    "child porn", "loli", "hentai", "nsfw",
    
    # Swear words & variations
    "fuck", "fucking", "fucked", "fucker", "fck", "fuk", "frick", "fricking",
    "shit", "shitty", "sht", "crap", "crappy", "damn", "dammit", "damm",
    "hell", "bitch", "bitching", "bastard", "asshole", "a$$hole",
    "motherfucker", "mf", "wtf", "stfu", "bullshit", "bs",
    
    # Drugs & alcohol
    "buy drug", "purchase drugs", "weed", "marijuana", "cocaine", "heroin", "meth",
    "ecstasy", "beer", "vodka",
    
    # Inappropriate references & people
    "diddy", "didy", "epstein", "weinstein", "cosby", "goon", "gooning",
    "onlyfans", "chaturbate", "pornhub",
    
    # Racial & hate speech
    "nigga", "nigger", "n1gga", "n1gger", "negro", "coon",
    "chink", "gook", "spic", "wetback", "beaner",
    "fag", "faggot", "dyke", "tranny", "trannie",
    "kkk", "white power", "supremacist",
    
    # Insults & bullying
    "retard", "retarded", "idiot", "stupid", "dumb", "moron",
    "ugly", "fat", "loser", "worthless", "pathetic", "weak",
    "whore", "slut", "hoe", "thot", "simp",
    
    # Scams & illegal
    "hack", "hacking", "cheat", "pirate", "steal", "scam",
    "bomb", "terrorist", "terrorism", "explosion",
    
    # Medical/adult only
    "abortion", "viagra", "cialis",
    
    # Bypasses & leetspeak
    "a$$", "b!tch", "sh!t", "fvck", "phuck", "azz",
]

def contains_inappropriate_content(text):
    """Check if text contains inappropriate words (including alternative spellings)"""
    # Remove special characters and convert to lowercase
    cleaned_text = re.sub(r'[^a-z0-9\s]', '', text.lower())
    
    # Check for inappropriate words
    for word in INAPPROPRIATE_WORDS:
        # Create pattern that matches word with any characters between letters
        pattern = ''.join(f'{char}[^a-z0-9]*' for char in word)
        if re.search(pattern, cleaned_text):
            return True
    
    return False

# Load model and tokenizer (cached so it only loads once)
@st.cache_resource
def load_model():
    """Load the FLAN-T5 model and tokenizer"""
    tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
    model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
    return tokenizer, model

# Load model
with st.spinner("Loading AI model... (this may take a moment on first run)"):
    tokenizer, model = load_model()

def get_homework_help(question):
    """Generate homework help using the model"""
    try:
        # Format prompt for educational response
        prompt = f"Explain step-by-step in a clear and formal way: {question}"
        
        # Tokenize input
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
        
        # Generate response
        outputs = model.generate(
            inputs.input_ids,
            max_length=200,
            num_beams=4,
            early_stopping=True,
            temperature=0.7
        )
        
        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
        
    except Exception as e:
        return "I'm having trouble processing this question. Please try rephrasing it."

# Initialize chat history
if 'history' not in st.session_state:
    st.session_state.history = []

# User input
user_input = st.text_input("üí≠ Ask your homework question:")

# Process input
if user_input:
    # Check for inappropriate content
    if contains_inappropriate_content(user_input):
        response = "Sorry, I can only help with appropriate content. Is there anything else you would like to ask me?"
    else:
        # Get AI response
        with st.spinner("Thinking..."):
            response = get_homework_help(user_input)
    
    # Add to history
    st.session_state.history.append({
        "question": user_input,
        "answer": response
    })

# Display chat history
if st.session_state.history:
    st.markdown("---")
    st.subheader("üìù Your Study Session")
    
    for i, chat in enumerate(st.session_state.history):
        st.markdown(f"**‚ùì Question {i+1}:** {chat['question']}")
        st.markdown(f"**‚úÖ Answer:** {chat['answer']}")
        st.markdown("")

# Sidebar
with st.sidebar:
    st.header("üìñ Subject Help")
    st.write("I can help with:")
    st.markdown("‚Ä¢ üî¢ Math")
    st.markdown("‚Ä¢ üî¨ Science")
    st.markdown("‚Ä¢ üìö English")
    st.markdown("‚Ä¢ üåç History")
    st.markdown("‚Ä¢ üíª Computer Science")
    st.markdown("‚Ä¢ And more!")
    
    st.markdown("---")
    st.write("**Rules:**")
    st.caption("‚úì Formal & clear explanations")
    st.caption("‚úì Step-by-step answers")
    st.caption("‚úì Appropriate content only")
    
    if st.button("üóëÔ∏è Clear Chat"):
        st.session_state.history = []
        st.rerun()

# Footer
st.markdown("---")
st.markdown("üöÄ Safe AI Homework Helper | Built for Students")
```

---

## **UPDATED FILE 2: `requirements.txt`**
```
streamlit
transformers
torch
